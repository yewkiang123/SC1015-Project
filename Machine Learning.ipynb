{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "sb.set()\n",
    "sb.set_palette(sb.color_palette(\"Set2\"))\n",
    "\n",
    "# Additional Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Import the Dataset\n",
    "df = pd.read_csv('datasets/student-dropout-academic-success-raw.csv')\n",
    "clean_df = df.drop(df[df.Target == \"Enrolled\"].index)\n",
    "student_df = clean_df.iloc[:,[1, 13, 14, 15, 16, 17, 22, 23, 28, 29, 34]].copy()\n",
    "student_df[\"Target\"] = student_df[\"Target\"].map({\n",
    "    \"Dropout\": 1,\n",
    "    \"Graduate\": 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = student_df.iloc[:, 0:10] # Predictors\n",
    "response = student_df.iloc[:, -1] # Response = Target\n",
    "\n",
    "# Partition Dataset into 2 random portions - 80% Train, 20% Test\n",
    "predictors_train, predictors_test, response_train, response_test = train_test_split(predictors, response, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set : (3539, 10) (3539,)\n",
      "Test Set  : (885, 10) (885,)\n"
     ]
    }
   ],
   "source": [
    "# Check sample sizes\n",
    "print(\"Train Set :\", predictors_train.shape, response_train.shape)\n",
    "print(\"Test Set  :\", predictors_test.shape, response_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Model\n",
    "\n",
    "**Models:**\n",
    "- Logistic Regression\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- AdaBoost\n",
    "- XGBoost\n",
    "\n",
    "**Metric:** \n",
    "- Accuracy Score with CV\n",
    "- Precision Score\n",
    "- Recall Score\n",
    "- Explained Variance (R^2)\n",
    "- Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logreg.fit(predictors_train, response_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients \t: a =  [[ 0.02384679  0.37421748 -1.58463013  0.16639068 -0.4497374   0.0297742\n",
      "   0.08579978  0.01161992 -0.29973872 -0.0791188 ]\n",
      " [ 0.01750594  0.26851145  0.34186066  0.03204492 -0.26790359 -0.02391971\n",
      "  -0.09799599  0.00962851 -0.05510726  0.06279351]\n",
      " [-0.04135273 -0.64272893  1.24276947 -0.1984356   0.71764099 -0.00585448\n",
      "   0.01219621 -0.02124844  0.35484599  0.01632529]]\n",
      "Intercept \t: b =  [ 2.03399156 -0.15349066 -1.8805009 ]\n"
     ]
    }
   ],
   "source": [
    "# Coefficients of the Logistic Regression line\n",
    "print('Coefficients \\t: a = ', logreg.coef_)\n",
    "print('Intercept \\t: b = ', logreg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train_pred = logreg.predict(predictors_train)\n",
    "response_test_pred = logreg.predict(predictors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance (R^2): 0.7428652161627578\n",
      "Mean Squared Error (MSE): 0.5046623339926533\n",
      "Root Mean Squared Error (RMSE): 0.710395899476238 \n",
      "\n",
      "Accuracy Score without CV:  0.7107344632768362\n",
      "Accuracy Score with CV:  0.7428674316992365\n",
      "Precision Score:  0.617612706164108\n",
      "Recall Score:  0.5924676648001097\n",
      "F1 Score:  0.5807360533358834\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = cross_val_score(logreg, predictors_train, response_train, cv=10)\n",
    "mse_train = np.mean(np.square(np.array(response_train) - np.array(response_train_pred)))\n",
    "\n",
    "print(\"Explained Variance (R^2):\", logreg.score(predictors_train, response_train))\n",
    "print(\"Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Root Mean Squared Error (RMSE):\", np.sqrt(mse_train), \"\\n\")\n",
    "\n",
    "print(\"Accuracy Score without CV: \",accuracy_score(response_test, response_test_pred))\n",
    "print(\"Accuracy Score with CV: \",scores.mean())\n",
    "print(\"Precision Score: \", precision_score(response_test, response_test_pred, average = 'macro'))\n",
    "print(\"Recall Score: \", recall_score(response_test, response_test_pred, average = 'macro'))\n",
    "print(\"F1 Score: \", f1_score(response_test, response_test_pred, average = 'macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(predictors_train, response_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train_pred = dectree.predict(predictors_train)\n",
    "response_test_pred = dectree.predict(predictors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance (R^2): 0.9819157954224357\n",
      "Mean Squared Error (MSE): 0.04944899689177734\n",
      "Root Mean Squared Error (RMSE): 0.22237130411043898 \n",
      "\n",
      "Accuracy Score without CV:  0.6621468926553672\n",
      "Accuracy Score with CV:  0.6634712952737633\n",
      "Precision Score:  0.6015466713579921\n",
      "Recall Score:  0.6001588418910226\n",
      "F1 Score:  0.6008109009571571\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(dectree, predictors_train, response_train, cv=10)\n",
    "mse_train = np.mean(np.square(np.array(response_train) - np.array(response_train_pred)))\n",
    "\n",
    "print(\"Explained Variance (R^2):\", dectree.score(predictors_train, response_train))\n",
    "#print(\"Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Root Mean Squared Error (RMSE):\", np.sqrt(mse_train), \"\\n\")\n",
    "\n",
    "#print(\"Accuracy Score without CV: \",accuracy_score(response_test, response_test_pred))\n",
    "print(\"Accuracy Score with CV: \",scores.mean())\n",
    "print(\"Precision Score: \", precision_score(response_test, response_test_pred, average = 'macro'))\n",
    "print(\"Recall Score: \", recall_score(response_test, response_test_pred, average = 'macro'))\n",
    "#print(\"F1 Score: \", f1_score(response_test, response_test_pred, average = 'macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, random_state=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randfclf = RandomForestClassifier(max_depth = 10, random_state = 0)\n",
    "randfclf.fit(predictors_train, response_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train_pred = randfclf.predict(predictors_train)\n",
    "response_test_pred = randfclf.predict(predictors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance (R^2): 0.8736931336535745\n",
      "Mean Squared Error (MSE): 0.25939530940943767\n",
      "Root Mean Squared Error (RMSE): 0.5093086582902726 \n",
      "\n",
      "Accuracy Score without CV:  0.7288135593220338\n",
      "Accuracy Score with CV:  0.7541652662409373\n",
      "Precision Score:  0.6596718963373168\n",
      "Recall Score:  0.6254292685627512\n",
      "F1 Score:  0.6274889756420006\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(randfclf, predictors_train, response_train, cv=10)\n",
    "mse_train = np.mean(np.square(np.array(response_train) - np.array(response_train_pred)))\n",
    "\n",
    "print(\"Explained Variance (R^2):\", randfclf.score(predictors_train, response_train))\n",
    "print(\"Mean Squared Error (MSE):\", mse_train)\n",
    "print(\"Root Mean Squared Error (RMSE):\", np.sqrt(mse_train), \"\\n\")\n",
    "\n",
    "print(\"Accuracy Score without CV: \",accuracy_score(response_test, response_test_pred))\n",
    "print(\"Accuracy Score with CV: \",scores.mean())\n",
    "print(\"Precision Score: \", precision_score(response_test, response_test_pred, average = 'macro'))\n",
    "print(\"Recall Score: \", recall_score(response_test, response_test_pred, average = 'macro'))\n",
    "print(\"F1 Score: \", f1_score(response_test, response_test_pred, average = 'macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
